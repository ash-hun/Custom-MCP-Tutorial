{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi Server MCP Client**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Common transport**\n",
    "### 단일 Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='Get the weather in a given city', args_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x13896b9c0>),\n",
      " StructuredTool(name='get_weather_korea', description='Get the weather in a given city', args_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weather_koreaArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x138d349a0>)]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    pprint.pprint(client.get_tools())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유지 Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='Get the weather in a given city', args_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x13896bf60>),\n",
      " StructuredTool(name='get_weather_korea', description='Get the weather in a given city', args_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weather_koreaArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x13896ab60>)]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# 1. Create a client\n",
    "client =  MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Initialize a model\n",
    "await client.__aenter__()\n",
    "\n",
    "# 3. Load Tools\n",
    "tools = client.get_tools()\n",
    "pprint.pprint(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SSE 통신\n",
    "Model Context Protocol을 Server Sent Events를 통해 구현한 프로토콜. HTTP를 통해 원격서비스에 연결할 수 있다.\n",
    "- MCP 서버에서 먼저 running되고 있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='Get the weather in a given city', args_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x138d34540>),\n",
      " StructuredTool(name='get_weather_korea', description='Get the weather in a given city', args_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weather_koreaArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x138d349a0>)]\n"
     ]
    }
   ],
   "source": [
    "# 단일 세션\n",
    "import pprint\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    pprint.pprint(client.get_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'bind_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     36\u001b[39m prompt = (\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a smart agent.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mUse `weather` tool to get the weather in a given city.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mAnswer in Korean.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 5. agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m agent = \u001b[43mcreate_react_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai_chat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m agent_response = \u001b[38;5;28;01mawait\u001b[39;00m agent.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is the weather in Seoul?\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     46\u001b[39m agent_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mcp_tutorial/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:154\u001b[39m, in \u001b[36m_convert_modifier_to_prompt.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     prompt = state_modifier\n\u001b[32m    153\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = prompt\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mcp_tutorial/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:654\u001b[39m, in \u001b[36mcreate_react_agent\u001b[39m\u001b[34m(model, tools, prompt, response_format, state_schema, config_schema, checkpointer, store, interrupt_before, interrupt_after, debug, version, name)\u001b[39m\n\u001b[32m    651\u001b[39m tool_calling_enabled = \u001b[38;5;28mlen\u001b[39m(tool_classes) > \u001b[32m0\u001b[39m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _should_bind_tools(model, tool_classes) \u001b[38;5;129;01mand\u001b[39;00m tool_calling_enabled:\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     model = \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBaseChatModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m(tool_classes)\n\u001b[32m    656\u001b[39m model_runnable = _get_prompt_runnable(prompt) | model\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# If any of the tools are configured to return_directly after running,\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# our graph needs to check if these were called\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'bind_tools'"
     ]
    }
   ],
   "source": [
    "# 유지 세션\n",
    "import pprint\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# OpenAI 클라이언트 생성\n",
    "client_openai = AsyncOpenAI()\n",
    "\n",
    "# OpenAI 모델을 langchain 형식으로 래핑\n",
    "async def openai_chat(messages):\n",
    "    response = await client_openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}] + [{\"role\": \"user\", \"content\": messages}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 1. Create a client\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Initialize a model\n",
    "await client.__aenter__()\n",
    "\n",
    "# 3. Load Tools\n",
    "tools = client.get_tools()\n",
    "\n",
    "# 4. Define Prompt\n",
    "prompt = (\n",
    "    'You are a smart agent.'\n",
    "    'Use `weather` tool to get the weather in a given city.'\n",
    "    'Answer in Korean.'\n",
    ")\n",
    "\n",
    "# 5. agent\n",
    "agent = create_react_agent(openai_chat, tools, prompt=prompt)\n",
    "\n",
    "agent_response = await agent.ainvoke({\"messages\": \"What is the weather in Seoul?\"})\n",
    "agent_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Std I/O 통신\n",
    "로컬 프로세스에 적합, 통신을 위한 표준 입출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='Get the weather in a given city', args_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x13a9351c0>)]\n",
      "{'messages': [HumanMessage(content='What is the weather in Seoul?', additional_kwargs={}, response_metadata={}, id='b02a8455-1ea3-49f7-aa86-047259c8e572'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_m3uJPeRL5BneNbwc4IbUUIzs', 'function': {'arguments': '{\"city\":\"Seoul\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 75, 'total_tokens': 91, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGimVPax69CdVwfZMxLDbQbt2oBl3', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1bbcc9c0-2cbf-4885-9980-bfe100bd3f25-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Seoul'}, 'id': 'call_m3uJPeRL5BneNbwc4IbUUIzs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 75, 'output_tokens': 16, 'total_tokens': 91, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content='The weather in Seoul is 흐림', name='get_weather', id='4952f485-c761-459e-9a95-fa0405fc04f5', tool_call_id='call_m3uJPeRL5BneNbwc4IbUUIzs'),\n",
      "              AIMessage(content='서울의 날씨는 흐림입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 105, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGimWCcdSGAzQqYaO980VNbQ52Th8', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe4c641a-6377-4bbe-811c-d909a6771873-0', usage_metadata={'input_tokens': 105, 'output_tokens': 11, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"/Users/choejaehun/Desktop/mcp_tutorial/.venv/bin/python\",\n",
    "    args=[\"mcp_server_local.py\"],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialization\n",
    "        await session.initialize()\n",
    "        \n",
    "        # Get Tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "        pprint.pprint(tools)\n",
    "        \n",
    "        # Create Agent\n",
    "        agent = create_react_agent(models, tools, prompt=prompt)\n",
    "        agent_response = await agent.ainvoke({\"messages\": \"What is the weather in Seoul?\"})\n",
    "        pprint.pprint(agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\\n  \"target\": \"apple\"\\n}', name='get_name'), tool_calls=None)\n",
      "Tool response: get_name\n",
      "Tool response: apple's name is ASH\n",
      "Final response: 사과의 이름은 ASH입니다.\n"
     ]
    }
   ],
   "source": [
    "# 유지 세션\n",
    "import pprint\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from openai import AsyncOpenAI\n",
    "import json\n",
    "\n",
    "# 1. Create a client\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"information\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Initialize a model\n",
    "await client.__aenter__()\n",
    "\n",
    "# 3. Load Tools\n",
    "tools = client.get_tools()\n",
    "\n",
    "# 4. Define system prompt\n",
    "system_prompt = (\n",
    "    'You are a smart agent.'\n",
    "    'Use `information` tool to get the color or name in a given target.'\n",
    "    'Answer in Korean.'\n",
    ")\n",
    "\n",
    "# 5. Create OpenAI client\n",
    "openai_client = AsyncOpenAI()\n",
    "\n",
    "# 6. Define function calling format\n",
    "function_calls = [\n",
    "    {\n",
    "        \"name\": tool.name,\n",
    "        \"description\": tool.description,\n",
    "        \"parameters\": tool.args_schema\n",
    "    }\n",
    "    for tool in tools\n",
    "]\n",
    "\n",
    "QUESTION = \"What is the name of the apple?\"\n",
    "\n",
    "# 7. Make API call\n",
    "response = await openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": QUESTION}\n",
    "    ],\n",
    "    functions=function_calls,\n",
    "    function_call=\"auto\"\n",
    ")\n",
    "\n",
    "# 8. Process the response\n",
    "# 이 코드는 OpenAI API의 응답을 처리하는 부분입니다.\n",
    "# OpenAI의 응답에서 첫 번째 메시지를 가져옵니다\n",
    "message = response.choices[0].message\n",
    "print(message)\n",
    "\n",
    "# function call이 있는 경우 (날씨 도구를 사용하라는 응답인 경우)\n",
    "if message.function_call:\n",
    "    # function call의 이름과 인자를 추출합니다\n",
    "    function_name = message.function_call.name\n",
    "    function_args = json.loads(message.function_call.arguments)\n",
    "    \n",
    "    # 요청된 도구를 찾아서 실행합니다\n",
    "    for tool in tools:\n",
    "        if tool.name == function_name:\n",
    "            # 도구를 비동기로 실행하고 결과를 받습니다\n",
    "            tool_response = await tool.ainvoke(function_args)\n",
    "            print(f\"Tool response: {function_name}\")\n",
    "            print(f\"Tool response: {tool_response}\")\n",
    "            \n",
    "            # 도구의 결과를 포함하여 최종 응답을 생성합니다\n",
    "            final_response = await openai_client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": QUESTION},\n",
    "                    {\"role\": \"assistant\", \"content\": None, \"function_call\": {\"name\": function_name, \"arguments\": message.function_call.arguments}},\n",
    "                    {\"role\": \"function\", \"name\": function_name, \"content\": tool_response}\n",
    "                ]\n",
    "            )\n",
    "            print(f\"Final response: {final_response.choices[0].message.content}\")\n",
    "else:\n",
    "    # function call이 없는 경우 직접 응답을 출력합니다\n",
    "    print(f\"Response: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from openai import AsyncOpenAI\n",
    "import json\n",
    "\n",
    "# 1. Create a client\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"information\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Initialize a model\n",
    "await client.__aenter__()\n",
    "\n",
    "# 3. Load Tools\n",
    "tools = client.get_tools()\n",
    "\n",
    "# 4. Define system prompt\n",
    "system_prompt = (\n",
    "    'You are a smart agent.'\n",
    "    'Use `information` tool to get the color or name in a given target.'\n",
    "    'Answer in Korean.'\n",
    ")\n",
    "\n",
    "# 5. Create OpenAI client\n",
    "openai_client = AsyncOpenAI()\n",
    "\n",
    "# 6. Define function calling format\n",
    "function_calls = [\n",
    "    {\n",
    "        \"name\": tool.name,\n",
    "        \"description\": tool.description,\n",
    "        \"parameters\": tool.args_schema\n",
    "    }\n",
    "    for tool in tools\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is the color of the apple?\"\n",
    "QUESTION1 = \"What is the name of the apple?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BGmgkSXHleQLlDTXdlBA5K83XVE2A', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\\n  \"target\": \"apple\"\\n}', name='get_color'), tool_calls=None))], created=1743340106, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=104, total_tokens=120, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\\n  \"target\": \"apple\"\\n}', name='get_color'), tool_calls=None)\n",
      "ChatCompletion(id='chatcmpl-BGmgkSXHleQLlDTXdlBA5K83XVE2A', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\\n  \"target\": \"apple\"\\n}', name='get_color'), tool_calls=None))], created=1743340106, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=104, total_tokens=120, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\\n  \"target\": \"apple\"\\n}', name='get_color'), tool_calls=None)\n",
      "Tool response: get_color\n",
      "Tool response: apple's color is BLUE\n",
      "Final response: 사과의 색깔은 파란색입니다.\n"
     ]
    }
   ],
   "source": [
    "# 7. Make API call\n",
    "response = await openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": QUESTION}\n",
    "    ],\n",
    "    functions=function_calls,\n",
    "    function_call=\"auto\"\n",
    ")\n",
    "\n",
    "# 8. Process the response\n",
    "# 이 코드는 OpenAI API의 응답을 처리하는 부분입니다.\n",
    "# OpenAI의 응답에서 첫 번째 메시지를 가져옵니다\n",
    "message = response.choices[0].message\n",
    "pprint.pprint(response)\n",
    "pprint.pprint(message)\n",
    "# 8. Process the response\n",
    "# 이 코드는 OpenAI API의 응답을 처리하는 부분입니다.\n",
    "# OpenAI의 응답에서 첫 번째 메시지를 가져옵니다\n",
    "message = response.choices[0].message\n",
    "pprint.pprint(response)\n",
    "pprint.pprint(message)\n",
    "\n",
    "if message.function_call:\n",
    "    # function call의 이름과 인자를 추출합니다\n",
    "    function_name = message.function_call.name\n",
    "    function_args = json.loads(message.function_call.arguments)\n",
    "    \n",
    "    # 요청된 도구를 찾아서 실행합니다\n",
    "    for tool in tools:\n",
    "        if tool.name == function_name:\n",
    "            # 도구를 비동기로 실행하고 결과를 받습니다\n",
    "            tool_response = await tool.ainvoke(function_args)\n",
    "            print(f\"Tool response: {function_name}\")\n",
    "            print(f\"Tool response: {tool_response}\")\n",
    "            \n",
    "            # 도구의 결과를 포함하여 최종 응답을 생성합니다\n",
    "            final_response = await openai_client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": QUESTION},\n",
    "                    {\"role\": \"assistant\", \"content\": None, \"function_call\": {\"name\": function_name, \"arguments\": message.function_call.arguments}},\n",
    "                    {\"role\": \"function\", \"name\": function_name, \"content\": tool_response}\n",
    "                ]\n",
    "            )\n",
    "            print(f\"Final response: {final_response.choices[0].message.content}\")\n",
    "else:\n",
    "    # function call이 없는 경우 직접 응답을 출력합니다\n",
    "    print(f\"Response: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BGmgr5SfJitOYSkSyQZFC02d10oUi', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\\n  \"target\": \"apple\"\\n}', name='get_name'), tool_calls=None))], created=1743340113, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=104, total_tokens=120, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\\n  \"target\": \"apple\"\\n}', name='get_name'), tool_calls=None)\n",
      "Tool response: get_name\n",
      "Tool response: apple's name is ASH\n",
      "Final response: 사과의 이름은 ASH입니다.\n"
     ]
    }
   ],
   "source": [
    "# 7. Make API call\n",
    "response = await openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": QUESTION1}\n",
    "    ],\n",
    "    functions=function_calls,\n",
    "    function_call=\"auto\"\n",
    ")\n",
    "\n",
    "# 8. Process the response\n",
    "# 이 코드는 OpenAI API의 응답을 처리하는 부분입니다.\n",
    "# OpenAI의 응답에서 첫 번째 메시지를 가져옵니다\n",
    "message = response.choices[0].message\n",
    "pprint.pprint(response)\n",
    "pprint.pprint(message)\n",
    "\n",
    "if message.function_call:\n",
    "    # function call의 이름과 인자를 추출합니다\n",
    "    function_name = message.function_call.name\n",
    "    function_args = json.loads(message.function_call.arguments)\n",
    "    \n",
    "    # 요청된 도구를 찾아서 실행합니다\n",
    "    for tool in tools:\n",
    "        if tool.name == function_name:\n",
    "            # 도구를 비동기로 실행하고 결과를 받습니다\n",
    "            tool_response = await tool.ainvoke(function_args)\n",
    "            print(f\"Tool response: {function_name}\")\n",
    "            print(f\"Tool response: {tool_response}\")\n",
    "            \n",
    "            # 도구의 결과를 포함하여 최종 응답을 생성합니다\n",
    "            final_response = await openai_client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": QUESTION1},\n",
    "                    {\"role\": \"assistant\", \"content\": None, \"function_call\": {\"name\": function_name, \"arguments\": message.function_call.arguments}},\n",
    "                    {\"role\": \"function\", \"name\": function_name, \"content\": tool_response}\n",
    "                ]\n",
    "            )\n",
    "            print(f\"Final response: {final_response.choices[0].message.content}\")\n",
    "else:\n",
    "    # function call이 없는 경우 직접 응답을 출력합니다\n",
    "    print(f\"Response: {message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
